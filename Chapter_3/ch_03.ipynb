{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This chapter covers attention mechanisms, the engine of LLMs:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/01.webp\" width=\"700px\">,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Attending to different parts of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 A simple self-attention mechanism without trainable weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section explains a very simplified variant of self-attention, which does not contain any trainable weights\n",
    "- This is purely for illustration purposes and NOT the attention mechanism that is used in transformers\n",
    "- The next section, section 3.3.2, will extend this simple attention mechanism to implement the real self-attention mechanism\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"700px\">,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "        [0.05, 0.80, 0.55]   # step     (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(inputs.shape)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (In this book, we follow the common machine learning and deep learning convention where training examples are represented as rows and feature values as columns; in the case of the tensor shown above, each row represents a word, and each column represents an embedding dimension)\n",
    "\n",
    "- The primary objective of this section is to demonstrate how the context vector \n",
    " is calculated using the second input sequence, `x^(2)`, as a `query token`\n",
    "\n",
    "- The figure depicts the initial step in this process, which involves calculating the attention scores ω between \n",
    " x^(2) (`query`) and all other input elements through a dot product operation\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/08.webp\" width=\"700px\">,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: torch.Size([3]) and attn_scores_2 shape: torch.Size([6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.5500, 0.8700, 0.6600]), tensor([0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1]             # A\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "print(f'query shape: {query.shape} and attn_scores_2 shape: {attn_scores_2.shape}')\n",
    "query, attn_scores_2\n",
    "\n",
    "#A The second input token serves as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "attn_scores_2 shape:  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# computing attention scores\n",
    "for i, x_i in enumerate(inputs):\n",
    "    #print(x_i.shape)\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)\n",
    "print(\"attn_scores_2 shape: \", attn_scores_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the context of self attention mechanisms, the dot product determines the extent to which elements in a\n",
    " sequence attend to each other: the higher the dot product, the higher the similarity and attention score between two elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, as shown in Figure 3.9, we normalize each of the attention scores that  we computed previously.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"700px\">,\n",
    "\n",
    "**Figure 3.9** - After computing the attention scores ω21 to ω2T with respect to the input query x(2) , the next step is to obtain the attention weights α21 to α2T by normalizing the attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main goal behind the normalization shown in Figure 3.9 is to obtain attention weights that sum up to 1. This normalization is a convention that is useful for interpretation and for maintaining training stability in an LLM. Here's a straightforward method for achieving this normalization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum attn weights:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weigths_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights: \", attn_weigths_2_tmp)\n",
    "print(\"Sum attn weights: \", attn_weigths_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In practice, it's more common and advisable to use the softmax function for normalization.\n",
    " This approach is better at managing extreme values and offers more favorable gradient\n",
    " properties during training. Below is a basic implementation of the softmax function for\n",
    " normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum attn weights:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weigths_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights: \", attn_weigths_2_naive)\n",
    "print(\"Sum attn weights: \", attn_weigths_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, the softmax function ensures that the attention weights are always positive.\n",
    " This makes the output interpretable as probabilities or relative importance, where higher\n",
    " weights indicate greater importance.\n",
    "\n",
    "- Note that this naive softmax implementation (softmax_naive) may encounter numerical\n",
    " instability problems, such as overflow and underflow, when dealing with large or small input\n",
    " values. Therefore, in practice, it's advisable to use the PyTorch implementation of softmax,\n",
    " which has been extensively optimized for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum attn weights:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weigths_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights: \", attn_weigths_2)\n",
    "print(\"Sum attn weights: \", attn_weigths_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we computed the normalized attention weights, we are ready for the final step\n",
    " illustrated in Figure 3.10: calculating the context vector z(2) by multiplying the embedded\n",
    " input tokens, x(i), with the corresponding attention weights and then summing the resulting\n",
    " vectors.\n",
    "\n",
    " <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"700px\">,\n",
    "\n",
    "  **Figure 3.10** The final step, after calculating and normalizing the attention scores to obtain the attention\n",
    " weights for query x(2) , is to compute the context vector z(2) . This context vector is a combination of all input\n",
    " vectors x(1) to x(T) weighted by the attention weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteartion i = 0 : tensor([0.0596, 0.0208, 0.1233])\n",
      "iteartion i = 1 : tensor([0.1904, 0.2277, 0.2803])\n",
      "iteartion i = 2 : tensor([0.3234, 0.4260, 0.4296])\n",
      "iteartion i = 3 : tensor([0.3507, 0.4979, 0.4705])\n",
      "iteartion i = 4 : tensor([0.4340, 0.5250, 0.4813])\n",
      "iteartion i = 5 : tensor([0.4419, 0.6515, 0.5683])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weigths_2[i] * x_i\n",
    "    print(f'iteartion i = {i} : {context_vec_2}')\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the previous section, we computed `attention weights` and the `context vector for input 2`,\n",
    " as shown in the highlighted row in Figure 3.11. Now, we are extending this computation to\n",
    " calculate attention weights and context vectors for all inputs.\n",
    " \n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/11.webp\" width=\"700px\">,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We follow the same three steps as before, as summarized in Figure 3.12, except that we\n",
    " make a few modifications in the code to compute all context vectors instead of only the\n",
    " second context vector, z(2).\n",
    "\n",
    "   <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/12.webp\" width=\"700px\">,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 1:** `compute attention scores for all pairs of inputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When computing the preceding attention score tensor, we used for-loops in Python.\n",
    " However, for-loops are generally slow, and we can achieve the same results using matrix\n",
    " multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T  # shape(6, 3) * (3, 6) = output shape(6, 6)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 2:** we now normalize each row so that the values in each row sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies\n",
    " the dimension of the input tensor along which the function will be computed. By setting\n",
    " dim=-1, we are instructing the softmax function to apply the normalization along the last\n",
    " dimension of the attn_scores tensor. If attn_scores is a 2D tensor (for example, with a\n",
    " shape of [rows, columns]), dim=-1 will normalize across the columns so that the values in\n",
    " each row (summing over the column dimension) sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_2_sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum( [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(f'row_2_sum: {row_2_sum}')\n",
    "print(f'All row sums: {attn_weights.sum(dim=-1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 3:** Using these attention weights we compute all context vectors via matrix multiplicaiton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vectors = attn_weights @ inputs\n",
    "print(all_context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can double-check that the code is correct by comparing the 2nd row with the context\n",
    " vector z(2) that we computed previously in section 3.3.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector:  tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector: \", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Based on the result, we can see that the previously calculated context_vec_2 matches the\n",
    " second row in the previous tensor exactly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:\n",
    "This concludes the code walkthrough of a simple self-attention mechanism. In the next\n",
    " section, we will add trainable weights, enabling the LLM to learn from data and improve its\n",
    " performance on specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we are implementing the self-attention mechanism that is used in the\n",
    " original transformer architecture, the GPT models, and most other popular LLMs. This self\n",
    "attention mechanism is also called scaled dot-product attention. Figure 3.13 provides a\n",
    " mental model illustrating how this self-attention mechanism fits into the broader context of\n",
    " implementing an LLM.\n",
    "\n",
    "    <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/13.webp\" width=\"700px\">,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The most notable difference is the introduction of weight matrices that are updated\n",
    " during model training. These trainable weight matrices are crucial so that the model\n",
    " (specifically, the attention module inside the model) can learn to produce \"good\" context\n",
    " vectors. (Note that we will train the LLM in chapter 5.)\n",
    " \n",
    " - We will tackle this self-attention mechanism in the two subsections. First, we will code it\n",
    " step-by-step as before. Second, we will organize the code into a compact Python class that\n",
    " can be imported into an LLM architecture, which we will code in chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing the attention weights step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will implement the self-attention mechanism step by step by introducing the three\n",
    " trainable weight matrices Wq, Wk, and Wv. These three matrices are used to project the\n",
    " embedded input tokens, x(i), into query, key, and value vectors as illustrated in Figure 3.14.\n",
    "\n",
    "    <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/14.webp\" width=\"700px\">,\n",
    "\n",
    "- Figure 3.14 In the first step of the self-attention mechanism with trainable weight matrices, we compute query\n",
    " (q), key (k), and value (v) vectors for input elements x. Similar to previous sections, we designate the second\n",
    " input, x(2) , as the query input. The query vector q(2) is obtained via matrix multiplication between the input x(2)\n",
    " and the weight matrix Wq.  Similarly, we obtain the key and value vectors via matrix multiplication involving the\n",
    " weight matrices Wk and Wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Earlier in section 3.3.1, we defined the second input element x(2) as the query when we\n",
    " computed the simplified attention weights to compute the context vector z(2). Later, in\n",
    " section 3.3.2, we generalized this to compute all context vectors z(1) ... z(T) for the six\n",
    "word input sentence \"Your journey starts with one step.\"\n",
    "\n",
    "- Similarly, we will start by computing only one context vector, z(2), for illustration\n",
    " purposes. In the next section, we will modify this code to calculate all context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]             #A\n",
    "d_in = inputs.shape[1]      #B\n",
    "d_out = 2                   #C\n",
    "\n",
    "#A The second input element\n",
    "#B The input embedding size, d = 3\n",
    "#C The output embedding size, d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that in GPT-like models, the input and output dimensions are usually the same, but for\n",
    " illustration purposes, to better follow the computation, we choose different input (d_in=3)\n",
    " and output (d_out=2) dimensions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we initialize the three weight matrices Wq, Wk, and Wv that are shown in Figure 3.14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note that we are setting requires_grad=False to reduce clutter in the outputs for\n",
    " illustration purposes, but if we were to use the weight matrices for model training, we\n",
    " would set requires_grad=True to update these matrices during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we compute the query, key, and value vectors as shown earlier in Figure 3.14:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEIGHT PARAMETERS VS ATTENTION WEIGHTS**\n",
    "\n",
    "- Note that in the weight matrices W, the term \"weight\" is short for \"weight\n",
    " parameters,\" the values of a neural network that are optimized during training. This\n",
    " is not to be confused with the attention weights. As we already saw in the previous\n",
    " section, attention weights determine the extent to which a context vector depends on\n",
    " the different parts of the input, i.e., to what extent the network focuses on different\n",
    " parts of the input.\n",
    "\n",
    "- In summary, weight parameters are the fundamental, learned coefficients that define\n",
    " the network's connections, while attention weights are dynamic, context-specific\n",
    " values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though our temporary goal is to only compute the one context vector, z(2), we still\n",
    " `require the key and value vectors for all input elements` `as they are involved in computing\n",
    " the attention weights with respect to the query q(2)`, as illustrated in Figure 3.14.\n",
    "    \n",
    "    We can obtain all keys and values via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(f'keys.shape: {keys.shape}')\n",
    "print(f'values.shape: {values.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second step is now to compute the attention scores, as shown in Figure 3.15.\n",
    "\n",
    "    <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/15.webp\" width=\"700px\">,\n",
    "\n",
    "- Figure 3.15 The attention score computation is a dot-product computation similar to what we have used in the\n",
    " simplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing\n",
    " the dot-product between the input elements but using the query and key obtained by transforming the inputs\n",
    " via the respective weight matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, let's compute the attention score ω22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "print(attn_scores_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Again, we can generalize this computation to all attention scores via matrix multiplication:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query_2 (2) @ (2, 6) => (6)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The third step is now going from the attention scores to the attention weights, as illustrated\n",
    " in Figure 3.16.\n",
    "\n",
    "    <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/16.webp\" width=\"700px\">,\n",
    "\n",
    "- Normalize these attention scores using softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The difference to earlier is  that we now scale the attention scores by dividing them by the square root of the\n",
    " embedding dimension of the keys, (note that taking the square root is mathematically the\n",
    " same as exponentiating by 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]  # taking embedding dimension\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > THE RATIONALE BEHIND SCALED-DOT PRODUCT ATTENTION\n",
    " \n",
    "- The reason for the normalization by the embedding dimension size is to improve the\n",
    " training performance by avoiding small gradients. For instance, when scaling up the\n",
    " embedding dimension, which is typically greater than thousand for GPT-like LLMs,\n",
    " large dot products can result in very small gradients during backpropagation due to\n",
    " the softmax function applied to them. As dot products increase, the softmax function\n",
    " behaves more like a step function, resulting in gradients nearing zero. These small\n",
    " gradients can drastically slow down learning or cause training to stagnate.\n",
    " \n",
    " - The scaling by the square root of the embedding dimension is the reason why this\n",
    " self-attention mechanism is also called scaled-dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, the final step is to compute the context vectors, as illustrated in Figure 3.17.\n",
    "\n",
    "    <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"700px\">,\n",
    "\n",
    "- **In the final step of the self-attention computation, we compute the context vector by combining all  value vectors via the attention weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Similar to section 3.3, where we computed the context vector as a weighted sum over the\n",
    " input vectors, we now compute the context vector as a weighted sum over the value\n",
    " vectors. Here, the attention weights serve as a weighting factor that weighs the respective\n",
    " importance of each value vector. Similar to section 3.3, we can use matrix multiplication to\n",
    " obtain the output in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attn_weights_2: torch.Size([6])\n",
      "Shape of values: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of attn_weights_2: {attn_weights_2.shape}')\n",
    "print(f'Shape of values: {values.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we only computed a `single context vector, z(2)`. In the next section, we will generalize the code to compute `all context vectors` in the input sequence, `z(1) to z(T)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WHY QUERY, KEY, AND VALUE?\n",
    " \n",
    "- The terms \"key,\" \"query,\" and \"value\" in the context of attention mechanisms are\n",
    " borrowed from the domain of information retrieval and databases, where similar\n",
    " concepts are used to store, search, and retrieve information.\n",
    " \n",
    "- A \"query\" is analogous to a search query in a database. It represents the current\n",
    " item (e.g., a word or token in a sentence) the model focuses on or tries to\n",
    " understand. The query is used to probe the other parts of the input sequence to\n",
    " determine how much attention to pay to them.\n",
    "\n",
    "- The \"key\" is like a database key used for indexing and searching. In the attention\n",
    " mechanism, each item in the input sequence (e.g., each word in a sentence) has an\n",
    " associated key. These keys are used to match with the query.\n",
    "\n",
    "- The \"value\" in this context is similar to the value in a key-value pair in a database. It\n",
    " represents the actual content or representation of the input items. Once the model\n",
    " determines which keys (and thus which parts of the input) are most relevant to the\n",
    " query (the current focus item), it retrieves the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous sections, we have gone through a lot of steps to compute the self-attention\n",
    " outputs. This was mainly done for illustration purposes so we could go through one step at\n",
    " a time. In practice, with the LLM implementation in the next chapter in mind, it is helpful to\n",
    " organize this code into a Python class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compact self-attention class\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The __init__ method initializes trainable weight matrices (W_query, W_key, and\n",
    " W_value) for queries, keys, and values, each transforming the input dimension d_in to an\n",
    " output dimension d_out.\n",
    "\n",
    "- During the forward pass, using the forward method, we compute the attention scores\n",
    " (attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
    " Finally, we create a context vector by weighting the values with these normalized attention\n",
    " scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)  # d_in = 3, d_out = 2\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Figure 3.18 summarizes the self-attention mechanism we just implemented.\n",
    "\n",
    "    <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"700px\">,\n",
    "\n",
    "- In self-attention, we transform the input vectors in the input matrix X with the three weight\n",
    " matrices, Wq, Wk, and Wv. Then, we compute the attention weight matrix based on the resulting queries `(Q)`\n",
    " and keys `(K)`. Using the attention weights and values `(V)`, we then compute the context vectors (Z). (For visual\n",
    " clarity, we focus on a single input text with n tokens in this figure, not a batch of multiple inputs. Consequently,\n",
    " the 3D input tensor is simplified to a 2D matrix in this context. This approach allows for a more straightforward\n",
    " visualization and understanding of the processes involved. Also, for consistency with later figures, the values in\n",
    " the attention matrix do not depict the real attention weights.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As shown in Figure 3.18, self-attention involves the trainable weight matrices Wq, Wk, and\n",
    " Wv. These matrices transform input data into queries, keys, and values, which are crucial\n",
    " components of the attention mechanism. As the model is exposed to more data during\n",
    " training, it adjusts these trainable weights, as we will see in upcoming chapters.\n",
    "\n",
    "- We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's\n",
    " nn.Linear layers, which effectively perform matrix multiplication when the bias units are\n",
    " disabled. Additionally, a significant advantage of using nn.Linear instead of manually\n",
    " implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    " initialization scheme, contributing to more stable and effective model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices since nn.Linear uses a more  sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.1 have done in the `exercise_solutions.ipynb` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the next section, we will make enhancements to the self-attention mechanism, focusing\n",
    "specifically on incorporating causal and multi-head elements. \n",
    "\n",
    "- The causal aspect involves\n",
    "modifying the attention mechanism to prevent the model from accessing future information\n",
    "in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words.\n",
    "\n",
    "- The multi-head component involves splitting the attention mechanism into multiple\n",
    "\"heads.\" Each head learns different aspects of the data, allowing the model to\n",
    "simultaneously attend to information from different representation subspaces at different\n",
    "positions. This improves the model's performance in complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we modify the standard self-attention mechanism to create a causal\n",
    " attention mechanism, which is essential for developing an LLM in the subsequent chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Causal attention, also known as masked attention, is a specialized form of self-attention.\n",
    " It restricts a model to only consider previous and current inputs in a sequence when\n",
    " processing any given token. This is in contrast to the standard self-attention mechanism,\n",
    " which allows access to the entire input sequence at once.\n",
    " \n",
    " - Consequently, when computing attention scores, the causal attention mechanism\n",
    " ensures that the model only factors in tokens that occur at or before the current token in\n",
    " the sequence.\n",
    " \n",
    " - To achieve this in GPT-like LLMs, for each token processed, we mask out the future\n",
    " tokens, which come after the current token in the input text, as illustrated in Figure 3.19.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"700px\">,\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Figure 3.19 In causal attention, we mask out the attention weights above the diagonal such that for a given  input, the LLM can't access future tokens when computing the context vectors using the attention weights. For  example, for the word \"journey\" in the second row, we only keep the attention weights for the words before  (\"Your\") and in the current position (\"journey\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As illustrated in Figure 3.19, we mask out the attention weights above the diagonal, and we\n",
    " normalize the non-masked attention weights, such that the attention weights sum to 1 in\n",
    " each row. In the next section, we will implement this masking and normalization procedure\n",
    " in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we implement the causal attention mask in code. We start with the\n",
    " procedure summarized in Figure 3.20.\n",
    "\n",
    " <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/20.webp\" width=\"700px\">,\n",
    "\n",
    " - Figure 3.20 One way to obtain the masked attention weight matrix in causal attention is to apply the softmax\n",
    " function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting\n",
    " matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In the first step illustrated in Figure 3.20, we compute the attention weights using the\n",
    " softmax function as we have done in previous sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)   #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "values = sa_v2.W_value(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)\n",
    "\n",
    "#A Reuse the query and key weight matrices of the SelfAttention_v2 object from the previous section for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can implement step 2 in Figure 3.20 using PyTorch's tril function to create a mask\n",
    " where the values above the diagonal are zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we can multiply this mask with the attention weights to zero out the values above the\n",
    " diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `third step` in Figure 3.20 is to renormalize the attention weights to sum up to 1 again in\n",
    " each row. We can achieve this by dividing each element in each row by the sum in each row\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1])\n",
      "row sums:\n",
      " tensor([[0.1921],\n",
      "        [0.3700],\n",
      "        [0.5357],\n",
      "        [0.6775],\n",
      "        [0.8415],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "\n",
      "masked attention weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "print(row_sums.shape)\n",
    "print(\"row sums:\\n\", row_sums)\n",
    "\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(\"\\nmasked attention weights: \\n\", masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFORMATION LEAKAGE**\n",
    "\n",
    "- When we apply a mask and then renormalize the attention weights, it might initially\n",
    " appear that information from future tokens (which we intend to mask) could still\n",
    " influence the current token because their values are part of the softmax calculation.\n",
    " However, the key insight is that when we renormalize the attention weights after\n",
    " masking, what we're essentially doing is recalculating the softmax over a smaller\n",
    " subset (since masked positions don't contribute to the softmax value).\n",
    "\n",
    "- The mathematical elegance of softmax is that despite initially including all positions\n",
    " in the denominator, after masking and renormalizing, the effect of the masked\n",
    " positions is nullified — they don't contribute to the softmax score in any meaningful\n",
    " way.\n",
    "- In simpler terms, after masking and renormalization, the distribution of attention\n",
    " weights is as if it was calculated only among the unmasked positions to begin with.\n",
    " This ensures there's no information leakage from future (or otherwise masked)\n",
    " tokens as we intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While we could be technically done with implementing causal attention at this point, we can take advantage of a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently in fewer steps, as shown in Figure 3.21\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/21.webp\" width=\"700px\">,\n",
    "\n",
    "- Figure 3.21 A more efficient way to obtain the masked attention weight matrix in causal attention is to mask  the attention scores with negative infinity values before applying the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The softmax function converts its inputs into a probability distribution. When negative\n",
    " infinity values (-∞) are present in a row, the softmax function treats them as zero\n",
    " probability. (Mathematically, this is because e-∞ approaches 0.)\n",
    "\n",
    "- We can implement this more efficient masking \"trick\" by creating a mask with 1's above\n",
    " the diagonal and then replacing these 1's with negative infinity (-inf) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
      "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
      "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
      "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
      "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
      "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see based on the output, the values in each row sum to 1, and no further\n",
    " normalization is necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now use the modified attention weights to compute the context vectors via `context_vec = attn_weights @ values`, as in section 3.4. However, in the next section, we first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Dropout in deep learning is a technique where randomly selected hidden layer units are\n",
    " ignored during training, effectively \"dropping\" them out. This method helps prevent\n",
    " overfitting by ensuring that a model does not become overly reliant on any specific set of\n",
    " hidden layer units. It's important to emphasize that dropout is only used during training\n",
    " and is disabled afterward.\n",
    " - In the transformer architecture, including models like GPT, dropout in the attention\n",
    " mechanism is typically applied in two specific areas: after calculating the attention scores\n",
    " or after applying the attention weights to the value vectors.\n",
    " - Here, we will apply the dropout mask after computing the attention weights, as\n",
    " illustrated in Figure 3.22, because it's the more common variant in practice.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"700px\">,\n",
    "\n",
    "\n",
    "- Figure 3.22 Using the causal attention mask (upper left), we apply an additional dropout mask (upper right) to zero out additional attention weights to reduce overfitting during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the following code example, we use a dropout rate of 50%, which means masking out\n",
    " half of the attention weights. (When we train the GPT model in later chapters, we will use a  lower dropout rate, such as 0.1 or 0.2.)\n",
    " \n",
    " In the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor  consisting of ones for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)  # we choose dropout rate of 50%\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 =2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Having gained an understanding of causal attention and dropout masking, we will\n",
    " develop a concise Python class in the following section. This class is designed to facilitate\n",
    " the efficient application of these two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Implementing a compact causal attention class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will now incorporate the causal attention and dropout modifications into\n",
    " the SelfAttention Python class we developed in section 3.4. This class will then serve as a\n",
    " template for developing multi-head attention in the upcoming section, which is the final\n",
    " attention class we implement in this chapter.\n",
    "- But before we begin, one more thing is to ensure that the code can handle batches\n",
    " consisting of more than one input so that the CausalAttention class supports the batch\n",
    " outputs produced by the data loader we implemented in chapter 2.\n",
    "- For simplicity, to simulate such batch inputs, we duplicate the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) #2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, num_tokens, d_in = batch.shape \n",
    "b, num_tokens, d_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)                                          #A\n",
    "        self.register_buffer(                                                       #A2\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)      #B\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape      #C\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)   #C\n",
    "        attn_scores.masked_fill_(                      #D\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # PyTorch automatically applies the mask to all batches at once.\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec  \n",
    "    \n",
    "#A Compared to the previous SelfAttention_v1 class, we added a dropout layer\n",
    "#B The register_buffer call is also a new addition (more information is provided in the following text)\n",
    "#C We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0)\n",
    "#D In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory copies\n",
    "#A2:\n",
    "    #register_buffer takes two argumentsname: a string specifying the name under which the buffer will be registered (in your case, 'mask').\n",
    "    #tensor: the tensor itself that you want to register.PyTorch automatically associates the string name passed to register_buffer with a class attribute. \n",
    "    #In your case, self.register_buffer('mask', tensor) makes the tensor available as self.mask.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- register_buffer is a special method in PyTorch, provided by the nn.Module class. It is used to register tensors with the model as \"buffers\".\n",
    "\n",
    "- What are buffers in PyTorch?\n",
    "Buffers are tensors that are stored inside the model, but are not model parameters. Unlike parameters (nn.Parameter), buffers do not require gradients to be calculated and are not updated during training. However, they are saved and loaded with the model, making them useful for storing things like:\n",
    "Masks, \n",
    "Constants, \n",
    "Indices,\n",
    "Normalizing coefficients, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While all added code lines should be familiar from previous sections, we now added a\n",
    " `self.register_buffer()` call in the __init__ method. The use of register_buffer in\n",
    " PyTorch is not strictly necessary for all use cases but offers several advantages here. For\n",
    " instance, when we use the CausalAttention class in our LLM, `buffers` are automatically\n",
    " `moved to the appropriate device (CPU or GPU) along with our model`, which will be relevant\n",
    " when training the LLM in future chapters. This means we don't need to manually ensure\n",
    " these tensors are on the same device as your model parameters, avoiding device mismatch\n",
    " errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape:  torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape: \", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting context vector is a 3D tensor where each token is now represented by a 2D\n",
    " embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Figure 3.23 provides a mental model that summarizes what we have accomplished so far.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/23.webp\" width=\"700px\">,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > As illustrated in Figure 3.23, in this section, we focused on the concept and implementation\n",
    " of causal attention in neural networks. In the next section, we will expand on this concept\n",
    " and `implement a multi-head attention module that implements several of such causal  attention mechanisms in parallel.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this final section of this chapter, we are extending the previously implemented causal\n",
    " attention class over multiple-heads. This is also called multi-head attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The term \"multi-head\" refers to dividing the attention mechanism into multiple \"heads,\"\n",
    " each operating independently. In this context, a `single causal attention module` can be\n",
    " considered `single-head attention`, where there is only one set of attention weights\n",
    " processing the input sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the following subsections, we will tackle this expansion from causal attention to multi\n",
    "head attention. The first subsection will intuitively build a multi-head attention module by\n",
    " stacking multiple CausalAttention modules for illustration purposes. The second\n",
    " subsection will then implement the same multi-head attention module in a more\n",
    " complicated but computationally more efficient way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism (depicted earlier in Figure 3.18 in section 3.4.1), each with its own weights, and then combining their outputs. Using multiple instances of the self attention mechanism can be computationally intensive, but it's crucial for the kind of complex pattern recognition that models like transformer-based LLMs are known for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3.24 illustrates the structure of a multi-head attention module, which consists of  multiple single-head attention modules, as previously depicted in Figure 3.18, stacked on  top of each other.\n",
    "\n",
    " <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/24.webp\" width=\"700px\">\n",
    "\n",
    "\n",
    "> Figure 3.24 The multi-head attention module in this figure depicts two single-head attention modules stacked\n",
    " on top of each other. So, instead of using a single matrix Wv for computing the value matrices, in a multi-head\n",
    " attention module with two heads, we now have two value weight matrices: Wv1 and Wv2. The same applies to\n",
    " the other weight matrices, Wq and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine\n",
    " into a single context vector matrix Z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - As mentioned before, the main idea behind multi-head attention is to run the attention\n",
    " mechanism multiple times (in parallel) with different, learned linear projections — the\n",
    " results of multiplying the input data (like the query, key, and value vectors in attention\n",
    " mechanisms) by a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper class to implement multi-head attention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via  num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4 dimensional context vectors (d_out*num_heads=4), as illustrated in Figure 3.25.\n",
    "\n",
    " <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/25.webp\" width=\"700px\">\n",
    "\n",
    "\n",
    ">Figure 3.25 Using the MultiHeadAttentionWrapper, we specified the number of attention heads\n",
    " (num_heads). If we set num_heads=2, as shown in this figure, we obtain a tensor with two sets of context\n",
    " vector matrices. In each context vector matrix, the rows represent the context vectors corresponding to the\n",
    " tokens, and the columns correspond to the embedding dimension specified via d_out=4. We concatenate\n",
    " these context vector matrices along the column dimension. Since we have 2 attention heads and an\n",
    " embedding dimension of 2, the final embedding dimension is 2 × 2 = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(f'context_vecs.shape: {context_vecs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first dimension of the resulting context_vecs tensor is 2 since we have two input texts\n",
    " (the input texts are duplicated, which is why the context vectors are exactly the same for\n",
    " those). The second dimension refers to the 6 tokens in each input. The third dimension\n",
    " refers to the 4-dimensional embedding of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2** have done in the `exercise_solutions.ipynb` file\n",
    "\n",
    "- Change the input arguments for the MultiHeadAttentionWrapper(...,\n",
    " num_heads=2) call such that the output context vectors are 2-dimensional instead of\n",
    " 4-dimensional while keeping the setting num_heads=2. Hint: You don't have to modify\n",
    " the class implementation; you just have to change one of the other input arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this section, we implemented a MultiHeadAttentionWrapper that combined multiple single-head attention modules. However, note that these are processed sequentially via [head(x) for head in self.heads] in the forward method. We can improve this implementation by processing the heads in parallel. One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication, as we will explore in the next sectio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous section, we created a MultiHeadAttentionWrapper to implement multi\n",
    "head attention by stacking multiple single-head attention modules. This was done by\n",
    " instantiating and combining several CausalAttention objects.\n",
    "\n",
    " - Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\n",
    " CausalAttention, we can combine both of these concepts into a single\n",
    " MultiHeadAttention class. Also, in addition to just merging the\n",
    " MultiHeadAttentionWrapper with the CausalAttention code, we will make some other\n",
    " modifications to implement multi-head attention more efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> In the `MultiHeadAttentionWrapper`, multiple heads are implemented by creating a list\n",
    " of `CausalAttention objects` `(self.heads`), each representing a separate attention head.\n",
    " The CausalAttention class independently performs the attention mechanism, and the\n",
    " results from each head are concatenated. In contrast, the following `MultiHeadAttention`\n",
    " class integrates the multi-head functionality within a single class. It splits the input into\n",
    " multiple heads by reshaping the projected query, key, and value tensors and then combines\n",
    " the results from these heads after computing attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An efficient multi-head attention class\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads \n",
    "        self.head_dim = d_out // num_heads                                       #A\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)                                  #B\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)                                                     #C\n",
    "        values = self.W_value(x)                                                 #C\n",
    "        queries = self.W_query(x)                                                #C\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)           #D\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)       #D\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)     #D\n",
    "\n",
    "        keys = keys.transpose(1, 2)                                              #E\n",
    "        queries = queries.transpose(1, 2)                                        #E\n",
    "        values = values.transpose(1, 2)                                          #E\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)                             #F\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]                   #G\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)                          #H\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)                    #I\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)   #J\n",
    "        context_vec = self.out_proj(context_vec)                                 #K\n",
    "        return context_vec\n",
    "\n",
    " #A Reduce the projection dim to match desired output dim\n",
    " #B Use a Linear layer to combine head outputs\n",
    " #C Tensor shape: (b, num_tokens, d_out)\n",
    " #D We implicitly split the matrix by adding a `num_heads` dimension. Then we unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    " #E Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    " #F Compute dot product for each head\n",
    " #G Mask truncated to the number of tokens\n",
    " #H Use the mask to fill attention scores\n",
    " #I Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    " #J Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    " #K Add an optional linear projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though the reshaping (.view) and transposing (.transpose) of tensors inside the\n",
    " MultiHeadAttention class looks very complicated, mathematically, the MultiHeadAttention class impelemnts the same concept as the MultiHeadAttentionWrapper earlier.\n",
    "\n",
    "\n",
    "- On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked\n",
    " multiple single-head attention layers that we combined into a multi-head attention layer.\n",
    "The MultiHeadAttention class takes an integrated approach. It starts with a multi-head\n",
    " layer and then internally splits this layer into individual attention heads, as illustrated in\n",
    " Figure 3.26\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"700px\">\n",
    "\n",
    "\n",
    "- Figure 3.26 In the MultiheadAttentionWrapper class with two attention heads, we initialized two weight\n",
    " matrices Wq1 and Wq2 and computed two query matrices Q1 and Q2 as illustrated at the top of this figure. In\n",
    " the MultiheadAttention class, we initialize one larger weight matrix Wq , only perform one matrix\n",
    " multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into Q1 and Q2 as\n",
    " shown at the bottom of this figure. We do the same for the keys and values, which are not shown to reduce\n",
    " visual clutter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The splitting of the query, key, and value tensors, as depicted in Figure 3.26, is achieved\n",
    " through tensor reshaping and transposing operations using PyTorch's .view and\n",
    " .transpose methods. The input is first transformed (via linear layers for queries, keys, and\n",
    " values) and then reshaped to represent multiple heads.\n",
    " \n",
    "- The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`, where\n",
    " `head_dim = d_out / num_heads`. This splitting is then achieved using the .view method: a\n",
    " tensor of dimensions `(b, num_tokens, d_out)` is reshaped to dimension `(b, num_tokens,num_heads, head_dim)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors are then transposed to bring the `num_heads` dimension before the num_tokens dimension, resulting in a shape of `(b, num_heads, num_tokens, head_dim)`. This transposition is crucial for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continuing with MultiHeadAttention, after computing the attention weights and context\n",
    " vectors, the context vectors from all heads are transposed back to the shape (b,\n",
    " num_tokens, num_heads, head_dim). These vectors are then reshaped (flattened) into the\n",
    " shape (b, num_tokens, d_out), effectively combining the outputs from all heads.\n",
    "\n",
    "- Additionally, we added a so-called `output projection layer (self.out_proj)` to\n",
    " MultiHeadAttention after combining the heads, which is not present in the\n",
    " CausalAttention class. `This output projection layer is not strictly necessary` (see the\n",
    " References section in Appendix B for more details), but it `is commonly used in many LLM architectures`, which is why we added it here for completeness.\n",
    "\n",
    "- Even though the `MultiHeadAttention class` looks more complicated than the\n",
    " MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors,\n",
    " it is more efficient. The reason is that we only need one matrix multiplication to compute\n",
    " the keys, for instance, `keys = self.W_key(x)` (the same is true for the queries and\n",
    " values). In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication,\n",
    " which is computationally one of the most expensive steps, for each attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MultiHeadAttention` class can be used similar to the `SelfAttention` and  `CausalAttention` classes we implemented earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape \n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(f'context_vecs.shape: {context_vecs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we implemented the `MultiHeadAttention` class that we will use in the\n",
    " upcoming sections when implementing and training the LLM itself. Note that while the code\n",
    " is fully functional, we used relatively small embedding sizes and numbers of attention\n",
    " heads to keep the outputs readable.\n",
    "\n",
    "- For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention\n",
    " heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion\n",
    " parameters) has 25 attention heads and a context vector embedding size of 1600. Note\n",
    " that the embedding sizes of the token inputs and context embeddings are the same in GPT\n",
    " models `(d_in = d_out)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **EXERCISE 3.3 INITIALIZING GPT-2 SIZE ATTENTION MODULES**\n",
    " \n",
    " - Using the MultiHeadAttention class, initialize a multi-head attention module that\n",
    " has the same number of attention heads as the smallest GPT-2 model (12 attention\n",
    " heads). Also ensure that you use the respective input and output embedding sizes\n",
    " similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a\n",
    " context length of 1024 tokens.\n",
    "\n",
    " - `This exercise was done in exercise_solution.ipynb file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention mechanisms transform input elements into enhanced context\n",
    " vector representations that incorporate information about all inputs. \n",
    "- A self-attention mechanism computes the context vector representation\n",
    " as a weighted sum over the inputs. \n",
    "- In a simplified attention mechanism, the attention weights are computed\n",
    " via dot products. \n",
    "- A dot product is just a concise way of multiplying two vectors element\n",
    "wise and then summing the products. \n",
    "- Matrix multiplications, while not strictly required, help us to implement\n",
    " computations more efficiently and compactly by replacing nested for\n",
    "loops. \n",
    "- In self-attention mechanisms that are used in LLMs, also called scaled-dot\n",
    " product attention, we include trainable weight matrices to compute\n",
    " intermediate transformations of the inputs: queries, values, and keys. \n",
    "- When working with LLMs that read and generate text from left to right,\n",
    " we add a causal attention mask to prevent the LLM from accessing future\n",
    " tokens. \n",
    "- Next to causal attention masks to zero out attention weights, we can also\n",
    " add a dropout mask to reduce overfitting in LLMs. \n",
    "- The attention modules in transformer-based LLMs involve multiple\n",
    " instances of causal attention, which is called multi-head attention. \n",
    "- We can create a multi-head attention module by stacking multiple\n",
    " instances of causal attention modules. \n",
    "- A more efficient way of creating multi-head attention modules involves\n",
    " batched matrix multiplications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
