{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import torch\n",
    "print(\"torch version:\", version(\"torch\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 4.1 NUMBER OF PARAMETERS IN FEED FORWARD AND ATTENTION MODULES**\n",
    " - Calculate and compare the number of parameters that are contained in the feed\n",
    " forward module and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import TransformerBlock\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in one multi-head attention module: 2,360,064\n",
      "Number of parameters in one feed forward module: 4,722,432\n"
     ]
    }
   ],
   "source": [
    "total_params_attn_block = sum(p.numel() for p in block.att.parameters())\n",
    "total_params_ffn_block = sum(p.numel() for p in block.ff.parameters())\n",
    "\n",
    "print(f'Number of parameters in one multi-head attention module: {total_params_attn_block:,}')\n",
    "print(f'Number of parameters in one feed forward module: {total_params_ffn_block:,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results above are for a single transformer block\n",
    "- Since, we have 12x repetaed transformer blocks in gpt2 architecture, we just need to multiply that params to 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in gpt2 multi-head attention module: 28,320,768\n",
      "Number of parameters in gpt2 feed forward module: 56,669,184\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of parameters in gpt2 multi-head attention module: {total_params_attn_block * 12:,}')\n",
    "print(f'Number of parameters in gpt2 feed forward module: {total_params_ffn_block * 12:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 4.2 INITIALIZING LARGER GPT MODELS**\n",
    "-  In this chapter, we initialized a 124 million parameter GPT model, which is known as\n",
    " \"GPT-2 small.\" Without making any code modifications besides updating the\n",
    " configuration file, use the GPTModel class to implement GPT-2 medium (using 1024\n",
    "dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads),\n",
    " GPT-2 large (1280-dimensional embeddings, 36 transformer blocks, 20 multi-head\n",
    " attention heads), and GPT-2 XL (1600-dimensional embeddings, 48 transformer\n",
    " blocks, 25 multi-head attention heads). As a bonus, calculate the total number of\n",
    " parameters in each GPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT2-small** (the 124M configuration we already implemented):\n",
    "- \"emb_dim\" = 768\n",
    "- \"n_layers\" = 12\n",
    "- \"n_heads\" = 12\n",
    "\n",
    "**GPT2-medium:**\n",
    "- \"emb_dim\" = 1024\n",
    "- \"n_layers\" = 24\n",
    "- \"n_heads\" = 16\n",
    "\n",
    "**GPT2-large:**\n",
    "- \"emb_dim\" = 1280\n",
    "- \"n_layers\" = 36\n",
    "- \"n_heads\" = 20\n",
    "\n",
    "**GPT2-XL:**\n",
    "- \"emb_dim\" = 1600\n",
    "- \"n_layers\" = 48\n",
    "- \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "def get_config(base_config, model_name=\"gpt2-small\"):\n",
    "    GPT_CONFIG = base_config.copy()\n",
    "\n",
    "    if model_name == \"gpt2-small\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 768\n",
    "        GPT_CONFIG[\"n_layers\"] = 12\n",
    "        GPT_CONFIG[\"n_heads\"] = 12\n",
    "\n",
    "    elif model_name == \"gpt2-medium\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1024\n",
    "        GPT_CONFIG[\"n_layers\"] = 24\n",
    "        GPT_CONFIG[\"n_heads\"] = 16\n",
    "\n",
    "    elif model_name == \"gpt2-large\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1280\n",
    "        GPT_CONFIG[\"n_layers\"] = 36\n",
    "        GPT_CONFIG[\"n_heads\"] = 20\n",
    "\n",
    "    elif model_name == \"gpt2-xl\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1600\n",
    "        GPT_CONFIG[\"n_layers\"] = 48\n",
    "        GPT_CONFIG[\"n_heads\"] = 25\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect model name {model_name}\")\n",
    "\n",
    "    return GPT_CONFIG\n",
    "\n",
    "def calculate_size(model): # based on chapter code\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "    total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "    print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "    \n",
    "    # Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "    total_size_bytes = total_params * 4\n",
    "    \n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "gpt2-small:\n",
      "Total number of parameters: 163,009,536\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n",
      "\n",
      "\n",
      "gpt2-medium:\n",
      "Total number of parameters: 406,212,608\n",
      "Number of trainable parameters considering weight tying: 354,749,440\n",
      "Total size of the model: 1549.58 MB\n",
      "\n",
      "\n",
      "gpt2-large:\n",
      "Total number of parameters: 838,220,800\n",
      "Number of trainable parameters considering weight tying: 773,891,840\n",
      "Total size of the model: 3197.56 MB\n",
      "\n",
      "\n",
      "gpt2-xl:\n",
      "Total number of parameters: 1,637,792,000\n",
      "Number of trainable parameters considering weight tying: 1,557,380,800\n",
      "Total size of the model: 6247.68 MB\n"
     ]
    }
   ],
   "source": [
    "from gpt import GPTModel\n",
    "\n",
    "\n",
    "for model_abbrev in (\"small\", \"medium\", \"large\", \"xl\"):\n",
    "    model_name = f\"gpt2-{model_abbrev}\"\n",
    "    CONFIG = get_config(GPT_CONFIG_124M, model_name=model_name)\n",
    "    model = GPTModel(CONFIG)\n",
    "    print(f\"\\n\\n{model_name}:\")\n",
    "    calculate_size(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
