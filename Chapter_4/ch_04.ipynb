{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Implementing a GPT model from Scratch To Generate Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers\n",
    "- Coding a GPT-like large language model (LLM) that can be trained to generate\n",
    "human-like text\n",
    "- Normalizing layer activations to stabilize neural network training\n",
    "- Adding shortcut connections in deep neural networks to train models more\n",
    "effectively\n",
    "- Implementing transformer blocks to create GPT models of various sizes\n",
    "- Computing the number of parameters and storage requirements of GPT models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we implement a GPT-like LLM architecture; the next chapter will focus on training this LLM\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chapter 1 discussed models like GPT and Llama, which generate words sequentially and are based on the decoder part of the original transformer architecture\n",
    "- Therefore, these LLMs are often referred to as \"decoder-like\" LLMs\n",
    "- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code\n",
    "- We'll see that many elements are repeated in an LLM's architecture\n",
    "\n",
    "Figure 4.2 provides a top-down view of a GPT-like LLM, with its main components highlighted.\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous chapters, we used smaller embedding dimensions for simplicity, ensuring\n",
    "that the concepts and examples could comfortably fit on a single page. Now, in this chapter,\n",
    "we are scaling up to the size of a small GPT-2 model, specifically the smallest version with\n",
    "124 million parameters, as described in Radford et al.'s paper, \"Language Models are\n",
    "Unsupervised Multitask Learners.\" Note that while the original report mentions 117 million\n",
    "parameters, this was later corrected\n",
    "\n",
    "- Chapter 6 will focus on loading pretrained weights into our implementation and adapting\n",
    "it for larger GPT-2 models with 345, 762, and 1,542 million parameters. In the context of\n",
    "deep learning and LLMs like GPT, the term \"parameters\" refers to the trainable weights of\n",
    "the model. These weights are essentially the internal variables of the model that are\n",
    "adjusted and optimized during the training process to minimize a specific loss function. This\n",
    "optimization allows the model to learn from the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GPT-2 VERSUS GPT-3`\n",
    "\n",
    "- Note that we are focusing on GPT-2 because OpenAI has made the weights of the\n",
    "pretrained model publicly available, which we will load into our implementation in\n",
    "chapter 6. GPT-3 is fundamentally the same in terms of model architecture, except\n",
    "that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion parameters in\n",
    "GPT-3, and it is trained on more data. As of this writing, the weights for GPT-3 are\n",
    "not publicly available. GPT-2 is also a better choice for learning how to implement\n",
    "LLMs, as it can be run on a single laptop computer, whereas GPT-3 requires a GPU\n",
    "cluster for training and inference. `According to Lambda Labs, it would take 355 years to train GPT-3 on a single V100 datacenter GPU, and 665 years on a consumer RTX 8000 GPU.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration details for the 124 million parameter GPT-2 model include:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"vocab_size\" refers to a vocabulary of 50,257 words, as used by the\n",
    "BPE tokenizer from chapter 2.\n",
    "- \"context_length\" denotes the maximum number of input tokens the\n",
    "model can handle, via the positional embeddings discussed in chapter 2.\n",
    "- \"emb_dim\" represents the embedding size, transforming each token into\n",
    "a 768-dimensional vector.\n",
    "- \"n_heads\" indicates the count of attention heads in the multi-head\n",
    "attention mechanism, as implemented in chapter 3.\n",
    "- \"n_layers\" specifies the number of transformer blocks in the model,\n",
    "which will be elaborated on in upcoming sections.\n",
    "- \"drop_rate\" indicates the intensity of the dropout mechanism (0.1\n",
    "implies a 10% drop of hidden units) to prevent overfitting, as covered in\n",
    "chapter 3.\n",
    "- \"qkv_bias\" determines whether to include a bias vector in the Linear\n",
    "layers of the multi-head attention for query, key, and value computations.\n",
    "We will initially disable this, following the norms of modern LLMs, but will\n",
    "revisit it in chapter 6 when we load pretrained GPT-2 weights from\n",
    "OpenAI into our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the configuration above, we will start this chapter by implementing a GPT placeholder\n",
    "architecture (DummyGPTModel) in this section, as shown in Figure 4.3. This will provide us\n",
    "with a big-picture view of how everything fits together and what other components we need\n",
    "to code in the upcoming sections to assemble the full GPT model architecture.\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"700px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])  #A\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])                    #B\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):         #C\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):                       #D\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyLayerNorm(nn.Module):                        #E\n",
    "    def __init__(self, normalized_shape, eps=1e-5):     #F\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):               \n",
    "        return x\n",
    "\n",
    "#A Use a placeholder for TransformerBlock\n",
    "#B Use a placeholder for LayerNorm\n",
    "#C A simple placeholder class that will be replaced by a real TransformerBlock later\n",
    "#D This block does nothing and just returns its input.\n",
    "#E A simple placeholder class that will be replaced by a real TransformerBlock later\n",
    "#F The parameters here are just to mimic the LayerNorm interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we will prepare the input data and initialize a new GPT model to illustrate its\n",
    "usage. Building on the figures we have seen in chapter 2, where we coded the tokenizer,\n",
    "Figure 4.4 provides a high-level overview of how data flows in and out of a GPT model.\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp\" width=\"700px\">\n",
    "\n",
    "Figure 4.4 A big-picture overview showing how the input data is tokenized, embedded, and fed to the GPT\n",
    "model. Note that in our DummyGPTClass coded earlier, the token embedding is handled inside the GPT model.\n",
    "In LLMs, the embedded input token dimension typically matches the output dimension. The output embeddings\n",
    "here represent the context vectors we discussed in chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To implement the steps shown in Figure 4.4, we tokenize a batch consisting of two text\n",
    "inputs for the GPT model using the tiktoken tokenizer introduced in chapter 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([2, 4])\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "print(len(batch))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch.shape)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0448,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output tensor has two rows corresponding to the two text samples. Each text sample\n",
    "consists of 4 tokens; each token is a 50,257-dimensional vector, which matches the size of\n",
    "the tokenizer's vocabulary.\n",
    "- The embedding has 50,257 dimensions because each of these dimensions refers to a\n",
    "unique token in the vocabulary. At the end of this chapter, when we implement the\n",
    "postprocessing code, we will convert these 50,257-dimensional vectors back into token IDs,\n",
    "which we can then decode into words.\n",
    "- Now that we have taken a top-down look at the GPT architecture and its in- and outputs,\n",
    "we will code the individual placeholders in the upcoming sections, starting with the real\n",
    "layer normalization class that will replace the DummyLayerNorm in the previous code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training deep neural networks with many layers can sometimes prove challenging due to\n",
    "issues like vanishing or exploding gradients. These issues lead to unstable training\n",
    "dynamics and make it difficult for the network to effectively adjust its weights, which\n",
    "means the learning process struggles to find a set of parameters (weights) for the neural\n",
    "network that minimizes the loss function. In other words, the network has difficulty learning\n",
    "the underlying patterns in the data to a degree that would allow it to make accurate\n",
    "predictions or decisions.\n",
    "- In this section, we will implement layer normalization to improve the stability and\n",
    "efficiency of neural network training.\n",
    "- The main idea behind layer normalization is to adjust the activations (outputs) of a\n",
    "neural network layer to have a mean of 0 and a variance of 1, also known as unit variance.\n",
    "This adjustment speeds up the convergence to effective weights and ensures consistent,\n",
    "reliable training. \n",
    "- Layer normalization is applied both before and after the multi-head attention module within the transformer block, which we will implement later; it's also applied before the final output layer\n",
    "- Before we implement layer normalization in code, Figure 4.5 provides a visual overview\n",
    "of how layer normalization functions.\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see how layer normalization works by passing a small input sample through a simple neural network layer:\n",
    "\n",
    "We can recreate the example shown in Figure 4.5 via the following code, where we\n",
    "implement a neural network layer with 5 inputs and 6 outputs that we apply to two input\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before we apply layer normalization to these outputs, let's examine the mean and\n",
    "variance:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(f'Mean:\\n {mean}')\n",
    "print(f'Variance:\\n {var}')\n",
    "mean.shape, var.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The normalization is applied to each of the two inputs (rows) independently; using dim=-1 applies the calculation across the last dimension (in this case, the feature dimension) instead of the row dimension\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/06.webp\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Later, when\n",
    "adding layer normalization to the GPT model, which produces 3D tensors with shape\n",
    "[batch_size, num_tokens, embedding_size], we can still use dim=-1 for normalization\n",
    "across the last dimension, avoiding a change from dim=1 to dim=2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [5.9605e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print('Mean:\\n', mean)\n",
    "print('Variance:\\n', var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each input is centered at 0 and has a unit variance of 1; to improve readability, we can disable PyTorch's scientific notation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print('Mean:\\n', mean)\n",
    "print('Variance:\\n', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above, we normalized the features of each input\n",
    "- Now, using the same idea, we can implement a LayerNorm class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer normalization class\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that in addition to performing the normalization by subtracting the mean and dividing by the variance, we added two trainable parameters, a scale and a shift parameter\n",
    "- The initial scale (multiplying by 1) and shift (adding 0) values don't have any effect; however, scale and shift are trainable parameters that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task\n",
    "- This allows the model to learn appropriate scaling and shifting that best suit the data it is processing\n",
    "- Note that we also add a smaller value (eps) before computing the square root of the variance; this is to avoid division-by-zero errors if the variance is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIASED VARIANCE**\n",
    "- In our variance calculation method, we have opted for an implementation detail by\n",
    "setting unbiased=False. For those curious about what this means, in the variance\n",
    "calculation, we divide by the number of inputs n in the variance formula. This\n",
    "approach does not apply Bessel's correction, which typically uses n-1 instead of n in\n",
    "the denominator to adjust for bias in sample variance estimation. This decision\n",
    "results in a so-called biased estimate of the variance. For large-scale language\n",
    "models (LLMs), where the embedding dimension n is significantly large, the\n",
    "difference between using n and n-1 is practically negligible. We chose this approach\n",
    "to ensure compatibility with the GPT-2 model's normalization layers and because it\n",
    "reflects TensorFlow's default behavior, which was used to implement the original GPT2 model. Using a similar setting ensures our method is compatible with the\n",
    "pretrained weights we will load in chapter 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try the LayerNorm module in practice and apply it to the batch input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we covered one of the building blocks we will need to implement the GPT\n",
    "architecture, as shown in the mental model in Figure 4.7\n",
    "\n",
    "  <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/07.webp\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the next section, we will look at the GELU activation function, which is one of the\n",
    "activation functions used in LLMs, instead of the traditional ReLU function we used in this\n",
    "section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAYER NORMALIZATION VERSUS BATCH NORMALIZATION**\n",
    "- If you are familiar with batch normalization, a common and traditional normalization\n",
    "method for neural networks, you may wonder how it compares to layer\n",
    "normalization. Unlike batch normalization, which normalizes across the batch\n",
    "dimension, layer normalization normalizes across the feature dimension. LLMs often\n",
    "require significant computational resources, and the available hardware or the\n",
    "specific use case can dictate the batch size during training or inference. Since layer\n",
    "normalization normalizes each input independently of the batch size, it offers more\n",
    "flexibility and stability in these scenarios. This is particularly beneficial for distributed\n",
    "training or when deploying models in environments where resources are constrained.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
